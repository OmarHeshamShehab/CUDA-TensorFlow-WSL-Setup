{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 13:03:27.588724: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-23 13:03:28.379281: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-23 13:03:30.815379: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking TensorFlow GPU availability...\n",
      "TensorFlow Version: 2.20.0\n",
      "GPU is available in TensorFlow: /device:GPU:0\n",
      "\n",
      "Checking CUDA installation...\n",
      "Found 1 CUDA device(s).\n",
      "Device 0: NVIDIA RTX A5000 Laptop GPU (Compute Capability: (8, 6))\n",
      "\n",
      "Checking TensorRT installation...\n",
      "TensorRT version: 10.13.3.9\n",
      "\n",
      "Checking cuDNN installation...\n",
      "cuDNN version information:\n",
      " #define CUDNN_MAJOR 9\n",
      "#define CUDNN_MINOR 3\n",
      "#define CUDNN_PATCHLEVEL 0\n",
      "--\n",
      "#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
      "\n",
      "/* cannot use constexpr here since this is a C-only file */\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758621816.089196    1228 gpu_device.cc:2020] Created device /device:GPU:0 with 13715 MB memory:  -> device: 0, name: NVIDIA RTX A5000 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1758621816.091160    1228 gpu_device.cc:2020] Created device /device:GPU:0 with 13715 MB memory:  -> device: 0, name: NVIDIA RTX A5000 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pycuda.driver as cuda\n",
    "import tensorrt as trt\n",
    "import os\n",
    "\n",
    "\n",
    "def check_tensorflow_gpu():\n",
    "    print(\"TensorFlow Version:\", tf.__version__)\n",
    "    if tf.test.gpu_device_name():\n",
    "        print(\"GPU is available in TensorFlow:\", tf.test.gpu_device_name())\n",
    "    else:\n",
    "        print(\"No GPU found in TensorFlow.\")\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    try:\n",
    "        cuda.init()\n",
    "        print(f\"Found {cuda.Device.count()} CUDA device(s).\")\n",
    "        for i in range(cuda.Device.count()):\n",
    "            device = cuda.Device(i)\n",
    "            print(\n",
    "                f\"Device {i}: {device.name()} (Compute Capability: {device.compute_capability()})\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(\"CUDA not found or failed to initialize:\", str(e))\n",
    "\n",
    "\n",
    "def check_tensorrt():\n",
    "    try:\n",
    "        print(\"TensorRT version:\", trt.__version__)\n",
    "    except Exception as e:\n",
    "        print(\"TensorRT not found:\", str(e))\n",
    "\n",
    "\n",
    "def check_cudnn():\n",
    "    try:\n",
    "        # Try both common locations\n",
    "        for path in [\n",
    "            \"/usr/include/cudnn_version.h\",\n",
    "            \"/usr/local/cuda/include/cudnn_version.h\",\n",
    "        ]:\n",
    "            if os.path.exists(path):\n",
    "                version_info = os.popen(f\"grep CUDNN_MAJOR -A 2 {path}\").read()\n",
    "                print(\"cuDNN version information:\\n\", version_info)\n",
    "                return\n",
    "        print(\"cuDNN header not found.\")\n",
    "    except Exception as e:\n",
    "        print(\"cuDNN check failed:\", str(e))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Checking TensorFlow GPU availability...\")\n",
    "    check_tensorflow_gpu()\n",
    "\n",
    "    print(\"\\nChecking CUDA installation...\")\n",
    "    check_cuda()\n",
    "\n",
    "    print(\"\\nChecking TensorRT installation...\")\n",
    "    check_tensorrt()\n",
    "\n",
    "    print(\"\\nChecking cuDNN installation...\")\n",
    "    check_cudnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
